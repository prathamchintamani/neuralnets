{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPBACnK4XQEhZKdqmsKsH8H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prathamchintamani/neuralnets/blob/main/iris_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "JGDl8wzJBg7u"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "else:\n",
        "  device = 'cpu'"
      ],
      "metadata": {
        "id": "xvFTf4bCSGdi"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "IktYHA64N5b_"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "# Create a DataFrame from the dataset\n",
        "iris_df = pd.DataFrame(iris.data)\n",
        "iris_df['class'] = iris.target\n",
        "iris_df.columns = ['sepal_len', 'sepal_wid', 'petal_len', 'petal_wid', 'class']"
      ],
      "metadata": {
        "id": "UDptgWT4Mvln"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = iris_df.sample(frac = 1)\n",
        "df_train,df_test = train_test_split(df,test_size = 0.1, random_state = 1)\n",
        "y_train = df_train['class']\n",
        "y_test = df_test['class']\n",
        "X_train = df_train.drop('class', axis = 1)\n",
        "X_test = df_test.drop('class', axis = 1)"
      ],
      "metadata": {
        "id": "xha3RmX1MxWJ"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_t,y_test_t,X_train_t,X_test_t = torch.from_numpy(y_train.to_numpy()),torch.from_numpy(y_test.to_numpy()),torch.from_numpy(X_train.to_numpy()),torch.from_numpy(X_test.to_numpy())"
      ],
      "metadata": {
        "id": "Y1IoW9SiNbXH"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_t = y_train_t.type(torch.long).to(device)\n",
        "y_test_t = y_test_t.type(torch.long).to(device)\n",
        "X_train_t = X_train_t.type(torch.float32).to(device)\n",
        "X_test_t = X_test_t.type(torch.float32).to(device)"
      ],
      "metadata": {
        "id": "pmjoFHbgP8s0"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class nn_model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.stack = nn.Sequential(\n",
        "        nn.Linear(4,4),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4,3)\n",
        "    )\n",
        "\n",
        "  def forward(self, X ):\n",
        "    return self.stack(X)"
      ],
      "metadata": {
        "id": "6Bg5jXStNjsZ"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_fn(y_true, y_pred):\n",
        "    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n",
        "    acc = (correct / len(y_pred)) * 100\n",
        "    return acc"
      ],
      "metadata": {
        "id": "4mP6048VRVRE"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0 = nn_model()"
      ],
      "metadata": {
        "id": "Ced6eh6OPw5N"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgpfatkUQQ6Y",
        "outputId": "2bc3aae1-c4fd-4f95-c93b-38960941af42"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nn_model(\n",
              "  (stack): Sequential(\n",
              "    (0): Linear(in_features=4, out_features=4, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=4, out_features=3, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model_0.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "2yhMQdfjQTTC"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "epochs = 1000\n"
      ],
      "metadata": {
        "id": "J5E-B0PtQpQj"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    ### Training\n",
        "    model_0.train()\n",
        "\n",
        "    # 1. Forward pass\n",
        "    y_logits = model_0(X_train_t) # model outputs raw logits\n",
        "    y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1) # go from logits -> prediction probabilities -> prediction labels\n",
        "    # print(y_logits)\n",
        "    # 2. Calculate loss and accuracy\n",
        "    loss = loss_fn(y_logits, y_train_t)\n",
        "    acc = accuracy_fn(y_true=y_train_t,\n",
        "                      y_pred=y_pred)\n",
        "\n",
        "    # 3. Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 4. Loss backwards\n",
        "    loss.backward()\n",
        "\n",
        "    # 5. Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    ### Testing\n",
        "    model_0.eval()\n",
        "    with torch.inference_mode():\n",
        "      # 1. Forward pass\n",
        "      test_logits = model_0(X_test_t)\n",
        "      test_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)\n",
        "      # 2. Calculate test loss and accuracy\n",
        "      test_loss = loss_fn(test_logits, y_test_t)\n",
        "      test_acc = accuracy_fn(y_true=y_test_t,\n",
        "                             y_pred=test_pred)\n",
        "\n",
        "    # Print out what's happening\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSHnFlBfQ7vM",
        "outputId": "c162f074-aa75-4d0f-b697-bdd341f6b4de"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Loss: 1.54913, Acc: 67.41% | Test Loss: 1.73177, Test Acc: 60.00%\n",
            "Epoch: 10 | Loss: 1.07956, Acc: 67.41% | Test Loss: 1.18138, Test Acc: 60.00%\n",
            "Epoch: 20 | Loss: 0.90943, Acc: 67.41% | Test Loss: 0.97641, Test Acc: 60.00%\n",
            "Epoch: 30 | Loss: 0.84551, Acc: 67.41% | Test Loss: 0.89818, Test Acc: 60.00%\n",
            "Epoch: 40 | Loss: 0.80704, Acc: 67.41% | Test Loss: 0.85681, Test Acc: 60.00%\n",
            "Epoch: 50 | Loss: 0.77433, Acc: 67.41% | Test Loss: 0.82652, Test Acc: 66.67%\n",
            "Epoch: 60 | Loss: 0.74393, Acc: 68.15% | Test Loss: 0.80040, Test Acc: 66.67%\n",
            "Epoch: 70 | Loss: 0.71537, Acc: 70.37% | Test Loss: 0.77644, Test Acc: 66.67%\n",
            "Epoch: 80 | Loss: 0.68863, Acc: 70.37% | Test Loss: 0.75401, Test Acc: 66.67%\n",
            "Epoch: 90 | Loss: 0.66371, Acc: 70.37% | Test Loss: 0.73291, Test Acc: 66.67%\n",
            "Epoch: 100 | Loss: 0.64061, Acc: 71.11% | Test Loss: 0.71310, Test Acc: 66.67%\n",
            "Epoch: 110 | Loss: 0.61945, Acc: 73.33% | Test Loss: 0.69442, Test Acc: 73.33%\n",
            "Epoch: 120 | Loss: 0.59996, Acc: 74.07% | Test Loss: 0.67712, Test Acc: 73.33%\n",
            "Epoch: 130 | Loss: 0.58202, Acc: 75.56% | Test Loss: 0.66100, Test Acc: 73.33%\n",
            "Epoch: 140 | Loss: 0.56559, Acc: 78.52% | Test Loss: 0.64588, Test Acc: 86.67%\n",
            "Epoch: 150 | Loss: 0.55066, Acc: 81.48% | Test Loss: 0.63177, Test Acc: 86.67%\n",
            "Epoch: 160 | Loss: 0.53706, Acc: 83.70% | Test Loss: 0.61858, Test Acc: 86.67%\n",
            "Epoch: 170 | Loss: 0.52456, Acc: 83.70% | Test Loss: 0.60638, Test Acc: 86.67%\n",
            "Epoch: 180 | Loss: 0.51308, Acc: 84.44% | Test Loss: 0.59506, Test Acc: 86.67%\n",
            "Epoch: 190 | Loss: 0.50246, Acc: 85.93% | Test Loss: 0.58446, Test Acc: 86.67%\n",
            "Epoch: 200 | Loss: 0.49258, Acc: 86.67% | Test Loss: 0.57459, Test Acc: 93.33%\n",
            "Epoch: 210 | Loss: 0.48336, Acc: 87.41% | Test Loss: 0.56528, Test Acc: 93.33%\n",
            "Epoch: 220 | Loss: 0.47480, Acc: 88.15% | Test Loss: 0.55623, Test Acc: 93.33%\n",
            "Epoch: 230 | Loss: 0.46678, Acc: 88.15% | Test Loss: 0.54778, Test Acc: 93.33%\n",
            "Epoch: 240 | Loss: 0.45923, Acc: 88.15% | Test Loss: 0.53985, Test Acc: 93.33%\n",
            "Epoch: 250 | Loss: 0.45208, Acc: 88.89% | Test Loss: 0.53230, Test Acc: 93.33%\n",
            "Epoch: 260 | Loss: 0.44530, Acc: 90.37% | Test Loss: 0.52508, Test Acc: 93.33%\n",
            "Epoch: 270 | Loss: 0.43884, Acc: 91.11% | Test Loss: 0.51819, Test Acc: 93.33%\n",
            "Epoch: 280 | Loss: 0.43268, Acc: 92.59% | Test Loss: 0.51147, Test Acc: 93.33%\n",
            "Epoch: 290 | Loss: 0.42678, Acc: 94.07% | Test Loss: 0.50500, Test Acc: 93.33%\n",
            "Epoch: 300 | Loss: 0.42113, Acc: 94.07% | Test Loss: 0.49870, Test Acc: 93.33%\n",
            "Epoch: 310 | Loss: 0.41570, Acc: 95.56% | Test Loss: 0.49249, Test Acc: 100.00%\n",
            "Epoch: 320 | Loss: 0.41046, Acc: 95.56% | Test Loss: 0.48645, Test Acc: 100.00%\n",
            "Epoch: 330 | Loss: 0.40541, Acc: 95.56% | Test Loss: 0.48055, Test Acc: 100.00%\n",
            "Epoch: 340 | Loss: 0.40051, Acc: 95.56% | Test Loss: 0.47481, Test Acc: 100.00%\n",
            "Epoch: 350 | Loss: 0.39576, Acc: 95.56% | Test Loss: 0.46924, Test Acc: 100.00%\n",
            "Epoch: 360 | Loss: 0.39113, Acc: 95.56% | Test Loss: 0.46381, Test Acc: 100.00%\n",
            "Epoch: 370 | Loss: 0.38662, Acc: 95.56% | Test Loss: 0.45848, Test Acc: 100.00%\n",
            "Epoch: 380 | Loss: 0.38222, Acc: 95.56% | Test Loss: 0.45324, Test Acc: 100.00%\n",
            "Epoch: 390 | Loss: 0.37791, Acc: 95.56% | Test Loss: 0.44808, Test Acc: 100.00%\n",
            "Epoch: 400 | Loss: 0.37370, Acc: 96.30% | Test Loss: 0.44295, Test Acc: 100.00%\n",
            "Epoch: 410 | Loss: 0.36957, Acc: 96.30% | Test Loss: 0.43787, Test Acc: 100.00%\n",
            "Epoch: 420 | Loss: 0.36552, Acc: 96.30% | Test Loss: 0.43284, Test Acc: 100.00%\n",
            "Epoch: 430 | Loss: 0.36154, Acc: 96.30% | Test Loss: 0.42791, Test Acc: 100.00%\n",
            "Epoch: 440 | Loss: 0.35763, Acc: 96.30% | Test Loss: 0.42305, Test Acc: 100.00%\n",
            "Epoch: 450 | Loss: 0.35378, Acc: 96.30% | Test Loss: 0.41826, Test Acc: 100.00%\n",
            "Epoch: 460 | Loss: 0.34999, Acc: 96.30% | Test Loss: 0.41352, Test Acc: 100.00%\n",
            "Epoch: 470 | Loss: 0.34626, Acc: 96.30% | Test Loss: 0.40883, Test Acc: 100.00%\n",
            "Epoch: 480 | Loss: 0.34257, Acc: 96.30% | Test Loss: 0.40419, Test Acc: 100.00%\n",
            "Epoch: 490 | Loss: 0.33894, Acc: 96.30% | Test Loss: 0.39959, Test Acc: 100.00%\n",
            "Epoch: 500 | Loss: 0.33536, Acc: 97.04% | Test Loss: 0.39504, Test Acc: 100.00%\n",
            "Epoch: 510 | Loss: 0.33182, Acc: 97.04% | Test Loss: 0.39053, Test Acc: 100.00%\n",
            "Epoch: 520 | Loss: 0.32832, Acc: 97.78% | Test Loss: 0.38606, Test Acc: 100.00%\n",
            "Epoch: 530 | Loss: 0.32487, Acc: 97.78% | Test Loss: 0.38163, Test Acc: 100.00%\n",
            "Epoch: 540 | Loss: 0.32146, Acc: 97.78% | Test Loss: 0.37724, Test Acc: 100.00%\n",
            "Epoch: 550 | Loss: 0.31810, Acc: 97.78% | Test Loss: 0.37284, Test Acc: 100.00%\n",
            "Epoch: 560 | Loss: 0.31478, Acc: 97.78% | Test Loss: 0.36848, Test Acc: 100.00%\n",
            "Epoch: 570 | Loss: 0.31150, Acc: 97.78% | Test Loss: 0.36419, Test Acc: 100.00%\n",
            "Epoch: 580 | Loss: 0.30826, Acc: 97.78% | Test Loss: 0.35992, Test Acc: 100.00%\n",
            "Epoch: 590 | Loss: 0.30506, Acc: 97.78% | Test Loss: 0.35571, Test Acc: 100.00%\n",
            "Epoch: 600 | Loss: 0.30189, Acc: 97.78% | Test Loss: 0.35156, Test Acc: 100.00%\n",
            "Epoch: 610 | Loss: 0.29877, Acc: 97.78% | Test Loss: 0.34745, Test Acc: 100.00%\n",
            "Epoch: 620 | Loss: 0.29568, Acc: 97.78% | Test Loss: 0.34339, Test Acc: 100.00%\n",
            "Epoch: 630 | Loss: 0.29263, Acc: 97.78% | Test Loss: 0.33937, Test Acc: 100.00%\n",
            "Epoch: 640 | Loss: 0.28962, Acc: 97.78% | Test Loss: 0.33540, Test Acc: 100.00%\n",
            "Epoch: 650 | Loss: 0.28665, Acc: 97.78% | Test Loss: 0.33144, Test Acc: 100.00%\n",
            "Epoch: 660 | Loss: 0.28371, Acc: 97.78% | Test Loss: 0.32755, Test Acc: 100.00%\n",
            "Epoch: 670 | Loss: 0.28081, Acc: 97.78% | Test Loss: 0.32370, Test Acc: 100.00%\n",
            "Epoch: 680 | Loss: 0.27795, Acc: 97.78% | Test Loss: 0.31987, Test Acc: 100.00%\n",
            "Epoch: 690 | Loss: 0.27513, Acc: 97.78% | Test Loss: 0.31611, Test Acc: 100.00%\n",
            "Epoch: 700 | Loss: 0.27234, Acc: 97.78% | Test Loss: 0.31240, Test Acc: 100.00%\n",
            "Epoch: 710 | Loss: 0.26959, Acc: 97.78% | Test Loss: 0.30873, Test Acc: 100.00%\n",
            "Epoch: 720 | Loss: 0.26688, Acc: 97.78% | Test Loss: 0.30511, Test Acc: 100.00%\n",
            "Epoch: 730 | Loss: 0.26420, Acc: 97.78% | Test Loss: 0.30154, Test Acc: 100.00%\n",
            "Epoch: 740 | Loss: 0.26155, Acc: 97.78% | Test Loss: 0.29802, Test Acc: 100.00%\n",
            "Epoch: 750 | Loss: 0.25895, Acc: 97.78% | Test Loss: 0.29454, Test Acc: 100.00%\n",
            "Epoch: 760 | Loss: 0.25638, Acc: 97.78% | Test Loss: 0.29110, Test Acc: 100.00%\n",
            "Epoch: 770 | Loss: 0.25384, Acc: 97.78% | Test Loss: 0.28771, Test Acc: 100.00%\n",
            "Epoch: 780 | Loss: 0.25134, Acc: 97.78% | Test Loss: 0.28437, Test Acc: 100.00%\n",
            "Epoch: 790 | Loss: 0.24888, Acc: 97.78% | Test Loss: 0.28105, Test Acc: 100.00%\n",
            "Epoch: 800 | Loss: 0.24645, Acc: 97.78% | Test Loss: 0.27778, Test Acc: 100.00%\n",
            "Epoch: 810 | Loss: 0.24406, Acc: 97.78% | Test Loss: 0.27457, Test Acc: 100.00%\n",
            "Epoch: 820 | Loss: 0.24170, Acc: 97.78% | Test Loss: 0.27140, Test Acc: 100.00%\n",
            "Epoch: 830 | Loss: 0.23938, Acc: 97.78% | Test Loss: 0.26828, Test Acc: 100.00%\n",
            "Epoch: 840 | Loss: 0.23709, Acc: 97.78% | Test Loss: 0.26520, Test Acc: 100.00%\n",
            "Epoch: 850 | Loss: 0.23484, Acc: 97.78% | Test Loss: 0.26217, Test Acc: 100.00%\n",
            "Epoch: 860 | Loss: 0.23262, Acc: 97.78% | Test Loss: 0.25919, Test Acc: 100.00%\n",
            "Epoch: 870 | Loss: 0.23043, Acc: 97.78% | Test Loss: 0.25623, Test Acc: 100.00%\n",
            "Epoch: 880 | Loss: 0.22828, Acc: 97.78% | Test Loss: 0.25333, Test Acc: 100.00%\n",
            "Epoch: 890 | Loss: 0.22616, Acc: 97.78% | Test Loss: 0.25047, Test Acc: 100.00%\n",
            "Epoch: 900 | Loss: 0.22408, Acc: 97.78% | Test Loss: 0.24766, Test Acc: 100.00%\n",
            "Epoch: 910 | Loss: 0.22202, Acc: 97.78% | Test Loss: 0.24489, Test Acc: 100.00%\n",
            "Epoch: 920 | Loss: 0.22000, Acc: 97.78% | Test Loss: 0.24217, Test Acc: 100.00%\n",
            "Epoch: 930 | Loss: 0.21801, Acc: 97.78% | Test Loss: 0.23949, Test Acc: 100.00%\n",
            "Epoch: 940 | Loss: 0.21605, Acc: 97.78% | Test Loss: 0.23686, Test Acc: 100.00%\n",
            "Epoch: 950 | Loss: 0.21413, Acc: 97.78% | Test Loss: 0.23426, Test Acc: 100.00%\n",
            "Epoch: 960 | Loss: 0.21223, Acc: 97.78% | Test Loss: 0.23171, Test Acc: 100.00%\n",
            "Epoch: 970 | Loss: 0.21036, Acc: 97.78% | Test Loss: 0.22920, Test Acc: 100.00%\n",
            "Epoch: 980 | Loss: 0.20853, Acc: 97.78% | Test Loss: 0.22672, Test Acc: 100.00%\n",
            "Epoch: 990 | Loss: 0.20672, Acc: 97.78% | Test Loss: 0.22429, Test Acc: 100.00%\n"
          ]
        }
      ]
    }
  ]
}