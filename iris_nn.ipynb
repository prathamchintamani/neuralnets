{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNzEtd4kTHtyhvz8mxIlt1g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prathamchintamani/neuralnets/blob/main/iris_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JGDl8wzJBg7u"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "else:\n",
        "  device = 'cpu'"
      ],
      "metadata": {
        "id": "xvFTf4bCSGdi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "IktYHA64N5b_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "# Create a DataFrame from the dataset\n",
        "iris_df = pd.DataFrame(iris.data)\n",
        "iris_df['class'] = iris.target\n",
        "iris_df.columns = ['sepal_len', 'sepal_wid', 'petal_len', 'petal_wid', 'class']"
      ],
      "metadata": {
        "id": "UDptgWT4Mvln"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = iris_df.sample(frac = 1)\n",
        "df_train,df_test = train_test_split(df,test_size = 0.1, random_state = 1)\n",
        "y_train = df_train['class']\n",
        "y_test = df_test['class']\n",
        "X_train = df_train.drop('class', axis = 1)\n",
        "X_test = df_test.drop('class', axis = 1)"
      ],
      "metadata": {
        "id": "xha3RmX1MxWJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_t,y_test_t,X_train_t,X_test_t = torch.from_numpy(y_train.to_numpy()),torch.from_numpy(y_test.to_numpy()),torch.from_numpy(X_train.to_numpy()),torch.from_numpy(X_test.to_numpy())"
      ],
      "metadata": {
        "id": "Y1IoW9SiNbXH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_t = y_train_t.type(torch.long).to(device)\n",
        "y_test_t = y_test_t.type(torch.long).to(device)\n",
        "X_train_t = X_train_t.type(torch.float32).to(device)\n",
        "X_test_t = X_test_t.type(torch.float32).to(device)"
      ],
      "metadata": {
        "id": "pmjoFHbgP8s0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class nn_model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.stack = nn.Sequential(\n",
        "        nn.Linear(4,4),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4,3)\n",
        "    )\n",
        "\n",
        "  def forward(self, X ):\n",
        "    return self.stack(X)"
      ],
      "metadata": {
        "id": "6Bg5jXStNjsZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_fn(y_true, y_pred):\n",
        "    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n",
        "    acc = (correct / len(y_pred)) * 100\n",
        "    return acc"
      ],
      "metadata": {
        "id": "4mP6048VRVRE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0 = nn_model()"
      ],
      "metadata": {
        "id": "Ced6eh6OPw5N"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgpfatkUQQ6Y",
        "outputId": "4ce75ae5-6407-4c93-bf95-3c3c72f34ada"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nn_model(\n",
              "  (stack): Sequential(\n",
              "    (0): Linear(in_features=4, out_features=4, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=4, out_features=3, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model_0.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "2yhMQdfjQTTC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "epochs = 1000\n"
      ],
      "metadata": {
        "id": "J5E-B0PtQpQj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    ### Training\n",
        "    model_0.train()\n",
        "\n",
        "    # 1. Forward pass\n",
        "    y_logits = model_0(X_train_t) # model outputs raw logits\n",
        "    y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1) # go from logits -> prediction probabilities -> prediction labels\n",
        "    # print(y_logits)\n",
        "    # 2. Calculate loss and accuracy\n",
        "    loss = loss_fn(y_logits, y_train_t)\n",
        "    acc = accuracy_fn(y_true=y_train_t,\n",
        "                      y_pred=y_pred)\n",
        "\n",
        "    # 3. Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 4. Loss backwards\n",
        "    loss.backward()\n",
        "\n",
        "    # 5. Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    ### Testing\n",
        "    model_0.eval()\n",
        "    with torch.inference_mode():\n",
        "      # 1. Forward pass\n",
        "      test_logits = model_0(X_test_t)\n",
        "      test_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)\n",
        "      # 2. Calculate test loss and accuracy\n",
        "      test_loss = loss_fn(test_logits, y_test_t)\n",
        "      test_acc = accuracy_fn(y_true=y_test_t,\n",
        "                             y_pred=test_pred)\n",
        "\n",
        "    # Print out what's happening\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSHnFlBfQ7vM",
        "outputId": "9a7420de-2e89-45c9-f90f-385909b7a707"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Loss: 1.64213, Acc: 35.56% | Test Loss: 1.16072, Test Acc: 33.33%\n",
            "Epoch: 10 | Loss: 0.84858, Acc: 79.26% | Test Loss: 0.80644, Test Acc: 86.67%\n",
            "Epoch: 20 | Loss: 0.65002, Acc: 78.52% | Test Loss: 0.60191, Test Acc: 93.33%\n",
            "Epoch: 30 | Loss: 0.53872, Acc: 91.85% | Test Loss: 0.49825, Test Acc: 100.00%\n",
            "Epoch: 40 | Loss: 0.47493, Acc: 96.30% | Test Loss: 0.43961, Test Acc: 100.00%\n",
            "Epoch: 50 | Loss: 0.42455, Acc: 97.04% | Test Loss: 0.39297, Test Acc: 100.00%\n",
            "Epoch: 60 | Loss: 0.38113, Acc: 96.30% | Test Loss: 0.38292, Test Acc: 93.33%\n",
            "Epoch: 70 | Loss: 0.55011, Acc: 64.44% | Test Loss: 0.75020, Test Acc: 46.67%\n",
            "Epoch: 80 | Loss: 0.41688, Acc: 67.41% | Test Loss: 0.55959, Test Acc: 46.67%\n",
            "Epoch: 90 | Loss: 0.42384, Acc: 67.41% | Test Loss: 0.57364, Test Acc: 46.67%\n",
            "Epoch: 100 | Loss: 0.40977, Acc: 69.63% | Test Loss: 0.55436, Test Acc: 53.33%\n",
            "Epoch: 110 | Loss: 0.40055, Acc: 72.59% | Test Loss: 0.54409, Test Acc: 53.33%\n",
            "Epoch: 120 | Loss: 0.38977, Acc: 72.59% | Test Loss: 0.52946, Test Acc: 60.00%\n",
            "Epoch: 130 | Loss: 0.38089, Acc: 74.07% | Test Loss: 0.51655, Test Acc: 60.00%\n",
            "Epoch: 140 | Loss: 0.37038, Acc: 77.78% | Test Loss: 0.49839, Test Acc: 60.00%\n",
            "Epoch: 150 | Loss: 0.36003, Acc: 80.00% | Test Loss: 0.48021, Test Acc: 66.67%\n",
            "Epoch: 160 | Loss: 0.35280, Acc: 80.74% | Test Loss: 0.46825, Test Acc: 66.67%\n",
            "Epoch: 170 | Loss: 0.33495, Acc: 81.48% | Test Loss: 0.44331, Test Acc: 66.67%\n",
            "Epoch: 180 | Loss: 0.32794, Acc: 82.22% | Test Loss: 0.43146, Test Acc: 66.67%\n",
            "Epoch: 190 | Loss: 0.30703, Acc: 83.70% | Test Loss: 0.39869, Test Acc: 73.33%\n",
            "Epoch: 200 | Loss: 0.28684, Acc: 85.19% | Test Loss: 0.36954, Test Acc: 80.00%\n",
            "Epoch: 210 | Loss: 0.27412, Acc: 85.93% | Test Loss: 0.35144, Test Acc: 86.67%\n",
            "Epoch: 220 | Loss: 0.26401, Acc: 87.41% | Test Loss: 0.32700, Test Acc: 93.33%\n",
            "Epoch: 230 | Loss: 0.25523, Acc: 88.15% | Test Loss: 0.30794, Test Acc: 93.33%\n",
            "Epoch: 240 | Loss: 0.25000, Acc: 88.15% | Test Loss: 0.29206, Test Acc: 93.33%\n",
            "Epoch: 250 | Loss: 0.24765, Acc: 88.15% | Test Loss: 0.31327, Test Acc: 93.33%\n",
            "Epoch: 260 | Loss: 0.23062, Acc: 88.89% | Test Loss: 0.27181, Test Acc: 93.33%\n",
            "Epoch: 270 | Loss: 0.23615, Acc: 88.89% | Test Loss: 0.28046, Test Acc: 93.33%\n",
            "Epoch: 280 | Loss: 0.22729, Acc: 89.63% | Test Loss: 0.28949, Test Acc: 93.33%\n",
            "Epoch: 290 | Loss: 0.21159, Acc: 90.37% | Test Loss: 0.25434, Test Acc: 93.33%\n",
            "Epoch: 300 | Loss: 0.21529, Acc: 89.63% | Test Loss: 0.25435, Test Acc: 93.33%\n",
            "Epoch: 310 | Loss: 0.17818, Acc: 94.07% | Test Loss: 0.18783, Test Acc: 93.33%\n",
            "Epoch: 320 | Loss: 0.12801, Acc: 97.04% | Test Loss: 0.12710, Test Acc: 93.33%\n",
            "Epoch: 330 | Loss: 0.15729, Acc: 94.81% | Test Loss: 0.18339, Test Acc: 93.33%\n",
            "Epoch: 340 | Loss: 0.16492, Acc: 94.07% | Test Loss: 0.16736, Test Acc: 93.33%\n",
            "Epoch: 350 | Loss: 0.13355, Acc: 95.56% | Test Loss: 0.13754, Test Acc: 93.33%\n",
            "Epoch: 360 | Loss: 0.15463, Acc: 94.07% | Test Loss: 0.14758, Test Acc: 93.33%\n",
            "Epoch: 370 | Loss: 0.13806, Acc: 95.56% | Test Loss: 0.14234, Test Acc: 93.33%\n",
            "Epoch: 380 | Loss: 0.14850, Acc: 95.56% | Test Loss: 0.15766, Test Acc: 93.33%\n",
            "Epoch: 390 | Loss: 0.13817, Acc: 95.56% | Test Loss: 0.13912, Test Acc: 93.33%\n",
            "Epoch: 400 | Loss: 0.15223, Acc: 94.81% | Test Loss: 0.15877, Test Acc: 93.33%\n",
            "Epoch: 410 | Loss: 0.14323, Acc: 95.56% | Test Loss: 0.14281, Test Acc: 93.33%\n",
            "Epoch: 420 | Loss: 0.14222, Acc: 95.56% | Test Loss: 0.13984, Test Acc: 93.33%\n",
            "Epoch: 430 | Loss: 0.13373, Acc: 95.56% | Test Loss: 0.12756, Test Acc: 93.33%\n",
            "Epoch: 440 | Loss: 0.11950, Acc: 95.56% | Test Loss: 0.10882, Test Acc: 93.33%\n",
            "Epoch: 450 | Loss: 0.10789, Acc: 97.04% | Test Loss: 0.09407, Test Acc: 93.33%\n",
            "Epoch: 460 | Loss: 0.10171, Acc: 97.04% | Test Loss: 0.08616, Test Acc: 93.33%\n",
            "Epoch: 470 | Loss: 0.09916, Acc: 97.04% | Test Loss: 0.08304, Test Acc: 93.33%\n",
            "Epoch: 480 | Loss: 0.09861, Acc: 97.04% | Test Loss: 0.08235, Test Acc: 93.33%\n",
            "Epoch: 490 | Loss: 0.09950, Acc: 97.04% | Test Loss: 0.08372, Test Acc: 93.33%\n",
            "Epoch: 500 | Loss: 0.10209, Acc: 97.04% | Test Loss: 0.08710, Test Acc: 93.33%\n",
            "Epoch: 510 | Loss: 0.10573, Acc: 97.04% | Test Loss: 0.09153, Test Acc: 93.33%\n",
            "Epoch: 520 | Loss: 0.10929, Acc: 96.30% | Test Loss: 0.09513, Test Acc: 93.33%\n",
            "Epoch: 530 | Loss: 0.10998, Acc: 96.30% | Test Loss: 0.09505, Test Acc: 93.33%\n",
            "Epoch: 540 | Loss: 0.10689, Acc: 96.30% | Test Loss: 0.09076, Test Acc: 93.33%\n",
            "Epoch: 550 | Loss: 0.10134, Acc: 97.04% | Test Loss: 0.08373, Test Acc: 93.33%\n",
            "Epoch: 560 | Loss: 0.09533, Acc: 97.04% | Test Loss: 0.07630, Test Acc: 93.33%\n",
            "Epoch: 570 | Loss: 0.09058, Acc: 97.04% | Test Loss: 0.07027, Test Acc: 93.33%\n",
            "Epoch: 580 | Loss: 0.08790, Acc: 97.04% | Test Loss: 0.06681, Test Acc: 93.33%\n",
            "Epoch: 590 | Loss: 0.08666, Acc: 97.04% | Test Loss: 0.06527, Test Acc: 93.33%\n",
            "Epoch: 600 | Loss: 0.08637, Acc: 97.04% | Test Loss: 0.06503, Test Acc: 93.33%\n",
            "Epoch: 610 | Loss: 0.08677, Acc: 97.04% | Test Loss: 0.06566, Test Acc: 93.33%\n",
            "Epoch: 620 | Loss: 0.08761, Acc: 97.04% | Test Loss: 0.06686, Test Acc: 93.33%\n",
            "Epoch: 630 | Loss: 0.08857, Acc: 97.04% | Test Loss: 0.06807, Test Acc: 93.33%\n",
            "Epoch: 640 | Loss: 0.08939, Acc: 97.04% | Test Loss: 0.06901, Test Acc: 93.33%\n",
            "Epoch: 650 | Loss: 0.08981, Acc: 97.04% | Test Loss: 0.06939, Test Acc: 93.33%\n",
            "Epoch: 660 | Loss: 0.08971, Acc: 97.04% | Test Loss: 0.06912, Test Acc: 93.33%\n",
            "Epoch: 670 | Loss: 0.08913, Acc: 97.04% | Test Loss: 0.06829, Test Acc: 93.33%\n",
            "Epoch: 680 | Loss: 0.08823, Acc: 97.04% | Test Loss: 0.06710, Test Acc: 93.33%\n",
            "Epoch: 690 | Loss: 0.08718, Acc: 97.04% | Test Loss: 0.06574, Test Acc: 93.33%\n",
            "Epoch: 700 | Loss: 0.08606, Acc: 97.04% | Test Loss: 0.06433, Test Acc: 93.33%\n",
            "Epoch: 710 | Loss: 0.08507, Acc: 97.04% | Test Loss: 0.06309, Test Acc: 93.33%\n",
            "Epoch: 720 | Loss: 0.08425, Acc: 97.04% | Test Loss: 0.06205, Test Acc: 93.33%\n",
            "Epoch: 730 | Loss: 0.08359, Acc: 97.04% | Test Loss: 0.06122, Test Acc: 93.33%\n",
            "Epoch: 740 | Loss: 0.08307, Acc: 97.04% | Test Loss: 0.06056, Test Acc: 93.33%\n",
            "Epoch: 750 | Loss: 0.08265, Acc: 97.04% | Test Loss: 0.06003, Test Acc: 93.33%\n",
            "Epoch: 760 | Loss: 0.08228, Acc: 97.04% | Test Loss: 0.05956, Test Acc: 93.33%\n",
            "Epoch: 770 | Loss: 0.08195, Acc: 97.04% | Test Loss: 0.05913, Test Acc: 93.33%\n",
            "Epoch: 780 | Loss: 0.08163, Acc: 97.04% | Test Loss: 0.05872, Test Acc: 93.33%\n",
            "Epoch: 790 | Loss: 0.08130, Acc: 97.04% | Test Loss: 0.05829, Test Acc: 93.33%\n",
            "Epoch: 800 | Loss: 0.08096, Acc: 97.04% | Test Loss: 0.05786, Test Acc: 93.33%\n",
            "Epoch: 810 | Loss: 0.08062, Acc: 97.04% | Test Loss: 0.05742, Test Acc: 93.33%\n",
            "Epoch: 820 | Loss: 0.08028, Acc: 97.04% | Test Loss: 0.05698, Test Acc: 93.33%\n",
            "Epoch: 830 | Loss: 0.07994, Acc: 97.04% | Test Loss: 0.05655, Test Acc: 93.33%\n",
            "Epoch: 840 | Loss: 0.07960, Acc: 97.04% | Test Loss: 0.05612, Test Acc: 93.33%\n",
            "Epoch: 850 | Loss: 0.07927, Acc: 97.04% | Test Loss: 0.05571, Test Acc: 93.33%\n",
            "Epoch: 860 | Loss: 0.07895, Acc: 97.04% | Test Loss: 0.05530, Test Acc: 93.33%\n",
            "Epoch: 870 | Loss: 0.07863, Acc: 97.04% | Test Loss: 0.05491, Test Acc: 93.33%\n",
            "Epoch: 880 | Loss: 0.07833, Acc: 97.04% | Test Loss: 0.05452, Test Acc: 93.33%\n",
            "Epoch: 890 | Loss: 0.07803, Acc: 97.04% | Test Loss: 0.05415, Test Acc: 93.33%\n",
            "Epoch: 900 | Loss: 0.07774, Acc: 97.04% | Test Loss: 0.05379, Test Acc: 93.33%\n",
            "Epoch: 910 | Loss: 0.07745, Acc: 97.04% | Test Loss: 0.05343, Test Acc: 93.33%\n",
            "Epoch: 920 | Loss: 0.07718, Acc: 97.04% | Test Loss: 0.05309, Test Acc: 93.33%\n",
            "Epoch: 930 | Loss: 0.07690, Acc: 97.04% | Test Loss: 0.05275, Test Acc: 93.33%\n",
            "Epoch: 940 | Loss: 0.07663, Acc: 97.04% | Test Loss: 0.05241, Test Acc: 93.33%\n",
            "Epoch: 950 | Loss: 0.07636, Acc: 97.04% | Test Loss: 0.05208, Test Acc: 93.33%\n",
            "Epoch: 960 | Loss: 0.07610, Acc: 97.04% | Test Loss: 0.05175, Test Acc: 93.33%\n",
            "Epoch: 970 | Loss: 0.07584, Acc: 97.04% | Test Loss: 0.05143, Test Acc: 100.00%\n",
            "Epoch: 980 | Loss: 0.07558, Acc: 97.04% | Test Loss: 0.05112, Test Acc: 100.00%\n",
            "Epoch: 990 | Loss: 0.07533, Acc: 97.04% | Test Loss: 0.05081, Test Acc: 100.00%\n"
          ]
        }
      ]
    }
  ]
}